---
title: 'Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock'
date: 2026-02-25
permalink: /posts/multilora
tags:
  - multi-LoRA
  - inference optimizations
  - vLLM
---

See our blog on [AWS](https://aws.amazon.com/blogs/machine-learning/efficiently-serve-dozens-of-fine-tuned-models-with-vllm-on-amazon-sagemaker-ai-and-amazon-bedrock/) and on [vLLM](https://blog.vllm.ai/2026/02/26/multi-lora.html) on our optimizations in vLLM for efficient multi-LoRA on MoE models, e.g., GPT-OSS and Qwen3-MoE.
